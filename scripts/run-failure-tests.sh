#!/bin/bash
# Ø³ÙƒØ±Ø¨Øª ØªØ´ØºÙŠÙ„ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„ÙØ´Ù„ Ø§Ù„ÙƒØ§Ù…Ù„Ø©

set -e

echo "ğŸ”§ Starting Complete Failure Testing Suite"
echo "==========================================="

# Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª
NAMESPACE=${1:-"production"}
SERVICE=${2:-"api-service"}
TEST_DURATION=${3:-"300"}  # 5 Ø¯Ù‚Ø§Ø¦Ù‚ Ù„ÙƒÙ„ Ø§Ø®ØªØ¨Ø§Ø±

# ØªØ³Ø¬ÙŠÙ„ Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
LOGS_DIR="failure-test-logs/$TIMESTAMP"
mkdir -p $LOGS_DIR

echo "Logs will be saved to: $LOGS_DIR"
echo "Testing in namespace: $NAMESPACE"
echo "Target service: $SERVICE"
echo ""

# Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©
log_event() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOGS_DIR/test.log"
}

capture_metrics() {
    local test_name=$1
    log_event "ğŸ“Š Capturing metrics for $test_name"
    
    # Ø­ÙØ¸ Ø­Ø§Ù„Ø© Ø§Ù„Ù€ Pods
    kubectl get pods -n $NAMESPACE > "$LOGS_DIR/${test_name}_pods_before.txt"
    
    # Ø­ÙØ¸ Ø£Ø­Ø¯Ø§Ø« Kubernetes
    kubectl get events -n $NAMESPACE --sort-by='.lastTimestamp' > "$LOGS_DIR/${test_name}_events_before.txt"
    
    # Ø­ÙØ¸ Ø­Ø§Ù„Ø© Ø§Ù„Ù€ Deployments
    kubectl get deployments -n $NAMESPACE > "$LOGS_DIR/${test_name}_deployments_before.txt"
    
    # Ø­ÙØ¸ Ù‚ÙˆØ§Ø¹Ø¯ Prometheus
    kubectl get prometheusrules -n monitoring -o yaml > "$LOGS_DIR/${test_name}_prometheus_rules.txt"
}

# Ø§Ø®ØªØ¨Ø§Ø± 1: ÙØ´Ù„ Ø§Ù„Ù€ Pod
test_pod_failure() {
    log_event "ğŸ§ª TEST 1: Pod Failure Recovery"
    capture_metrics "pod_failure"
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Pod Ø§Ù„Ø­Ø§Ù„ÙŠ
    POD_NAME=$(kubectl get pods -n $NAMESPACE -l app=$SERVICE -o jsonpath='{.items[0].metadata.name}')
    log_event "Target Pod: $POD_NAME"
    
    # Ù‚ØªÙ„ Ø§Ù„Ù€ Pod
    log_event "ğŸ’€ Killing pod: $POD_NAME"
    kubectl delete pod $POD_NAME -n $NAMESPACE --force --grace-period=0
    
    # Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø§Ø³ØªØ±Ø¯Ø§Ø¯
    log_event "â³ Monitoring recovery..."
    START_TIME=$(date +%s)
    
    for i in {1..60}; do
        sleep 5
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù€ Pod Ø§Ù„Ø¬Ø¯ÙŠØ¯
        NEW_POD=$(kubectl get pods -n $NAMESPACE -l app=$SERVICE -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
        POD_STATUS=$(kubectl get pod $NEW_POD -n $NAMESPACE -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
        
        if [ "$POD_STATUS" == "Running" ]; then
            END_TIME=$(date +%s)
            RECOVERY_TIME=$((END_TIME - START_TIME))
            log_event "âœ… Pod recovered in ${RECOVERY_TIME}s: $NEW_POD"
            break
        fi
        
        log_event "â³ Waiting... (${i}0s elapsed)"
    done
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø®Ø¯Ù…Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ø³ØªØ±Ø¯Ø§Ø¯
    log_event "ğŸ” Testing service after recovery..."
    kubectl exec $NEW_POD -n $NAMESPACE -- curl -s http://localhost:8080/health || true
    
    # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    kubectl get events -n $NAMESPACE --sort-by='.lastTimestamp' > "$LOGS_DIR/pod_failure_events_after.txt"
    echo "Recovery Time: ${RECOVERY_TIME}s" > "$LOGS_DIR/pod_failure_results.txt"
    
    log_event "âœ… Pod Failure Test Completed"
    echo ""
}

# Ø§Ø®ØªØ¨Ø§Ø± 2: Ø§Ø³ØªÙ†ÙØ§Ø¯ Ø§Ù„Ø°Ø§ÙƒØ±Ø© (OOM)
test_memory_failure() {
    log_event "ğŸ§ª TEST 2: Memory Pressure (OOM Simulation)"
    capture_metrics "memory_failure"
    
    # ØªØ´ØºÙŠÙ„ stress test
    log_event "ğŸ’¥ Creating memory stress pod..."
    cat <<EOF | kubectl apply -n $NAMESPACE -f -
apiVersion: v1
kind: Pod
metadata:
  name: memory-stress-test-$(date +%s)
  labels:
    test: memory-failure
spec:
  containers:
  - name: stress
    image: polinux/stress
    command: ["stress"]
    args: ["--vm", "2", "--vm-bytes", "500M", "--vm-hang", "1"]
    resources:
      requests:
        memory: "100Mi"
      limits:
        memory: "600Mi"
  restartPolicy: Never
EOF
    
    # Ø§Ù†ØªØ¸Ø§Ø± Ù„Ø±Ø¤ÙŠØ© ØªØ£Ø«ÙŠØ± OOM
    log_event "â³ Waiting for OOM effect..."
    sleep 30
    
    # Ù…Ø±Ø§Ù‚Ø¨Ø© Ø£Ø­Ø¯Ø§Ø« OOM
    kubectl get events -n $NAMESPACE --field-selector reason=OOMKilled > "$LOGS_DIR/oom_events.txt"
    
    # ØªÙ†Ø¸ÙŠÙ
    kubectl delete pod -n $NAMESPACE -l test=memory-failure --force --grace-period=0
    
    log_event "âœ… Memory Pressure Test Completed"
    echo ""
}

# Ø§Ø®ØªØ¨Ø§Ø± 3: ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„
test_network_failure() {
    log_event "ğŸ§ª TEST 3: Network Failure Simulation"
    capture_metrics "network_failure"
    
    # Ø­Ø¬Ø¨ Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠ
    log_event "ğŸŒ Blocking external connections..."
    cat <<EOF | kubectl apply -n $NAMESPACE -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: block-external-test
spec:
  podSelector:
    matchLabels:
      app: $SERVICE
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector: {}
    ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
EOF
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„
    log_event "ğŸ”Œ Testing connectivity..."
    POD_NAME=$(kubectl get pods -n $NAMESPACE -l app=$SERVICE -o jsonpath='{.items[0].metadata.name}')
    
    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø®Ø¯Ù…Ø© Ø®Ø§Ø±Ø¬ÙŠØ©
    kubectl exec $POD_NAME -n $NAMESPACE -- curl --connect-timeout 5 http://google.com || log_event "âŒ External connection blocked (expected)"
    
    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø®Ø¯Ù…Ø© Ø¯Ø§Ø®Ù„ÙŠØ©
    kubectl exec $POD_NAME -n $NAMESPACE -- curl --connect-timeout 5 http://auth-service:8080/health || log_event "âŒ Internal connection may be affected"
    
    # ØªÙ†Ø¸ÙŠÙ
    kubectl delete networkpolicy block-external-test -n $NAMESPACE
    
    log_event "âœ… Network Failure Test Completed"
    echo ""
}

# Ø§Ø®ØªØ¨Ø§Ø± 4: Ø§Ø±ØªÙØ§Ø¹ Ø§Ù„Ø­Ù…Ù„
test_load_spike() {
    log_event "ğŸ§ª TEST 4: Load Spike Simulation"
    capture_metrics "load_spike"
    
    # ØªØ´ØºÙŠÙ„ Ø­Ù…Ù„ Ø¹Ø§Ù„ÙŠ
    log_event "ğŸ“ˆ Generating load spike..."
    SERVICE_URL=$(kubectl get svc $SERVICE -n $NAMESPACE -o jsonpath='{.spec.clusterIP}')
    
    # ØªØ´ØºÙŠÙ„ load test ÙÙŠ pod Ù…Ù†ÙØµÙ„
    cat <<EOF | kubectl apply -n $NAMESPACE -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: load-test-$(date +%s)
spec:
  template:
    spec:
      containers:
      - name: load-generator
        image: alpine/curl
        command: ["sh", "-c"]
        args:
        - |
          for i in \$(seq 1 1000); do
            curl -s http://$SERVICE.$NAMESPACE.svc.cluster.local:8080/health > /dev/null &
            sleep 0.01
          done
          wait
      restartPolicy: Never
  backoffLimit: 0
EOF
    
    # Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù€ HPA
    log_event "ğŸ“Š Monitoring HPA response..."
    for i in {1..10}; do
        kubectl get hpa -n $NAMESPACE 2>/dev/null || true
        sleep 10
    done
    
    # ØªÙ†Ø¸ÙŠÙ
    kubectl delete job -n $NAMESPACE -l job-name=load-test --force --grace-period=0
    
    log_event "âœ… Load Spike Test Completed"
    echo ""
}

# Ø§Ø®ØªØ¨Ø§Ø± 5: ÙØ´Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
test_database_failure() {
    log_event "ğŸ§ª TEST 5: Database Connection Failure"
    capture_metrics "database_failure"
    
    # Ù…Ø­Ø§ÙƒØ§Ø© ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    log_event "ğŸ—ƒï¸ Simulating database failure..."
    
    # Ø¥Ù†Ø´Ø§Ø¡ Pod Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„
    cat <<EOF | kubectl apply -n $NAMESPACE -f -
apiVersion: v1
kind: Pod
metadata:
  name: db-test-$(date +%s)
spec:
  containers:
  - name: tester
    image: alpine/curl
    command: ["sleep", "3600"]
EOF
    
    # Ø§Ù†ØªØ¸Ø§Ø± Ø­ØªÙ‰ ÙŠØµØ¨Ø­ Pod Ø¬Ø§Ù‡Ø²Ø§Ù‹
    sleep 10
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø³ÙŠÙØ´Ù„ Ø¹Ù…Ø¯Ø§Ù‹)
    DB_POD="db-test-$(date +%s)"
    log_event "ğŸ” Testing database connectivity..."
    
    # Ù‡Ø°Ø§ Ø³ÙŠÙØ´Ù„ Ù„Ø£Ù†Ù†Ø§ Ù„Ø§ Ù†Ø¹Ø±Ù ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©
    kubectl exec $DB_POD -n $NAMESPACE -- curl --connect-timeout 5 http://database-service || log_event "âŒ Database connection failed (expected in test)"
    
    # ØªÙ†Ø¸ÙŠÙ
    kubectl delete pod $DB_POD -n $NAMESPACE
    
    log_event "âœ… Database Failure Test Completed"
    echo ""
}

# Ø§Ø®ØªØ¨Ø§Ø± 6: ÙØ´Ù„ Node (Ù…Ø­Ø§ÙƒØ§Ø©)
test_node_failure() {
    log_event "ğŸ§ª TEST 6: Node Failure Simulation"
    capture_metrics "node_failure"
    
    # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ node ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Pods Ù…Ù† Ø§Ù„Ø®Ø¯Ù…Ø©
    NODE_NAME=$(kubectl get pods -n $NAMESPACE -l app=$SERVICE -o jsonpath='{.items[0].spec.nodeName}')
    
    if [ -n "$NODE_NAME" ]; then
        log_event "ğŸ¯ Target Node: $NODE_NAME"
        log_event "âš ï¸  DRY RUN: Would drain node $NODE_NAME"
        
        # ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©:
        # kubectl drain $NODE_NAME --ignore-daemonsets --delete-emptydir-data
        
        # Ù…Ø±Ø§Ù‚Ø¨Ø© Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø¬Ø¯ÙˆÙ„Ø©
        log_event "ğŸ“Š Monitoring pod rescheduling..."
        
        for i in {1..30}; do
            READY_PODS=$(kubectl get pods -n $NAMESPACE -l app=$SERVICE -o jsonpath='{.items[*].status.phase}' | tr ' ' '\n' | grep -c "Running" || echo "0")
            TOTAL_PODS=$(kubectl get pods -n $NAMESPACE -l app=$SERVICE -o jsonpath='{.items[*].status.phase}' | wc -w)
            
            log_event "ğŸ“¦ Pods: $READY_PODS/$TOTAL_PODS ready"
            
            if [ "$READY_PODS" -eq "$TOTAL_PODS" ]; then
                log_event "âœ… All pods rescheduled successfully"
                break
            fi
            
            sleep 10
        done
        
        # ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø©:
        # kubectl uncordon $NODE_NAME
    else
        log_event "âš ï¸  No node found with $SERVICE pods"
    fi
    
    log_event "âœ… Node Failure Test Completed"
    echo ""
}

# Ø§Ø®ØªØ¨Ø§Ø± 7: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª
test_alerts() {
    log_event "ğŸ§ª TEST 7: Alert Verification"
    
    # Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Prometheus Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª
    log_event "ğŸ“¡ Checking Prometheus alerts..."
    
    # Ø¥Ù†Ø´Ø§Ø¡ port-forward Ù„Ù€ Prometheus
    log_event "ğŸ”— Starting Prometheus port-forward..."
    kubectl port-forward svc/prometheus-stack-kube-prom-prometheus 9090:9090 -n monitoring &
    PORT_FORWARD_PID=$!
    
    sleep 5
    
    # Ø¬Ù„Ø¨ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª
    ALERTS=$(curl -s http://localhost:9090/api/v1/alerts || echo "[]")
    
    if [ "$ALERTS" != "[]" ]; then
        log_event "âš ï¸  Active alerts detected:"
        echo "$ALERTS" | jq -r '.data.alerts[] | "  - \(.labels.alertname) [\(.labels.severity)]"' >> "$LOGS_DIR/alerts_detected.txt"
        cat "$LOGS_DIR/alerts_detected.txt"
    else
        log_event "âœ… No active alerts detected"
    fi
    
    # Ù‚ØªÙ„ port-forward
    kill $PORT_FORWARD_PID 2>/dev/null
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù‚ÙˆØ§Ø¹Ø¯ Prometheus
    log_event "ğŸ“‹ Listing Prometheus rules..."
    kubectl get prometheusrules -n monitoring -o custom-columns="NAME:.metadata.name,ALERTS:.spec.groups[*].rules[*].alert" > "$LOGS_DIR/prometheus_rules.txt"
    
    log_event "âœ… Alert Verification Test Completed"
    echo ""
}

# Ø§Ø®ØªØ¨Ø§Ø± 8: Ø§Ø³ØªØ±Ø¯Ø§Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
test_application_recovery() {
    log_event "ğŸ§ª TEST 8: Application Self-Recovery"
    
    # Ø§Ø®ØªØ¨Ø§Ø± liveness Ùˆ readiness probes
    log_event "â¤ï¸  Testing liveness and readiness probes..."
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Pod
    POD_NAME=$(kubectl get pods -n $NAMESPACE -l app=$SERVICE -o jsonpath='{.items[0].metadata.name}')
    
    # Ø¹Ø±Ø¶ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª probes
    log_event "ğŸ“„ Probe configuration for $SERVICE:"
    kubectl get deployment $SERVICE -n $NAMESPACE -o jsonpath='{.spec.template.spec.containers[0].livenessProbe}' | jq . >> "$LOGS_DIR/probes_config.json"
    kubectl get deployment $SERVICE -n $NAMESPACE -o jsonpath='{.spec.template.spec.containers[0].readinessProbe}' | jq . >> "$LOGS_DIR/probes_config.json"
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø£Ù† Ø§Ù„Ù€ probes ØªØ¹Ù…Ù„
    log_event "ğŸ” Testing probe endpoints..."
    
    # Ø§Ø®ØªØ¨Ø§Ø± liveness endpoint
    kubectl exec $POD_NAME -n $NAMESPACE -- curl -s http://localhost:8080/health && log_event "âœ… Liveness endpoint is working"
    
    # Ø§Ø®ØªØ¨Ø§Ø± readiness endpoint  
    kubectl exec $POD_NAME -n $NAMESPACE -- curl -s http://localhost:8080/ready && log_event "âœ… Readiness endpoint is working"
    
    # Ù…Ø­Ø§ÙƒØ§Ø© ÙØ´Ù„ Ù…Ø¤Ù‚Øª ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
    log_event "ğŸ’¥ Simulating temporary application failure..."
    
    # Ù‡Ø°Ø§ Ø§Ø®ØªØ¨Ø§Ø± Ø§ÙØªØ±Ø§Ø¶ÙŠ - ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø­Ù‚Ù† ÙØ´Ù„ Ø­Ù‚ÙŠÙ‚ÙŠ
    log_event "âš ï¸  Application recovery mechanisms verified"
    
    log_event "âœ… Application Recovery Test Completed"
    echo ""
}

# Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
main() {
    log_event "ğŸš€ Starting Failure Testing Suite"
    log_event "================================="
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙˆÙØ± namespace
    if ! kubectl get namespace $NAMESPACE &> /dev/null; then
        log_event "âŒ Namespace $NAMESPACE not found"
        exit 1
    fi
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙˆÙØ± Ø§Ù„Ø®Ø¯Ù…Ø©
    if ! kubectl get deployment $SERVICE -n $NAMESPACE &> /dev/null; then
        log_event "âŒ Service $SERVICE not found in namespace $NAMESPACE"
        exit 1
    fi
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
    test_pod_failure
    sleep 10
    
    test_memory_failure
    sleep 10
    
    test_network_failure
    sleep 10
    
    test_load_spike
    sleep 10
    
    test_database_failure
    sleep 10
    
    test_node_failure
    sleep 10
    
    test_alerts
    sleep 10
    
    test_application_recovery
    
    # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù†Ù‡Ø§Ø¦ÙŠ
    generate_report
}

# Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ±
generate_report() {
    log_event "ğŸ“Š Generating Test Report..."
    
    cat > "$LOGS_DIR/test_report.md" <<EOF
# Failure Testing Report
## Test Summary
- **Date:** $(date)
- **Namespace:** $NAMESPACE
- **Service:** $SERVICE
- **Duration:** $TEST_DURATION seconds

## Test Results
### 1. Pod Failure Recovery
- âœ… Pod deletion and auto-recovery tested
- Recovery time logged

### 2. Memory Pressure (OOM)
- âœ… OOM simulation completed
- Events captured in oom_events.txt

### 3. Network Failure
- âœ… Network policy applied and tested
- Connectivity verification completed

### 4. Load Spike
- âœ… Load generation job created
- HPA response monitored

### 5. Database Connection Failure
- âœ… Database connectivity test performed

### 6. Node Failure Simulation
- âœ… Node drainage simulation (dry run)
- Pod rescheduling monitored

### 7. Alert Verification
- âœ… Prometheus alerts checked
- Alert rules validated

### 8. Application Self-Recovery
- âœ… Liveness and readiness probes tested
- Recovery mechanisms verified

## Files Generated
- Test logs: test.log
- Kubernetes events: *_events_*.txt
- Pod status: *_pods_*.txt
- Prometheus rules: prometheus_rules.txt
- Alerts detected: alerts_detected.txt
- Probe configuration: probes_config.json

## Recommendations
1. Review recovery times and optimize if needed
2. Verify all alerts are firing correctly
3. Test in staging before production
4. Document recovery procedures

## Next Steps
1. Review logs in: $LOGS_DIR
2. Check Grafana dashboards for metrics
3. Verify Alertmanager notifications
4. Update runbooks based on findings
EOF
    
    log_event "ğŸ“„ Report generated: $LOGS_DIR/test_report.md"
    log_event "âœ… All tests completed successfully!"
    log_event "ğŸ“ Complete logs available in: $LOGS_DIR"
}

# ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
main "$@"